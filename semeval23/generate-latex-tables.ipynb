{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18cbabf-41c0-4593-aa2c-d9755125438a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def fm(i, k):\n",
    "    return '{0:.2f}'.format(i[k])\n",
    "\n",
    "def table_task_1(team_name):\n",
    "    df = f'/workspace/all-submissions/{team_name}/evaluation-task-1.jsonl'\n",
    "    if not os.path.isfile(df):\n",
    "        return \"\"\n",
    "    \n",
    "    df = pd.read_json(df, lines=True)\n",
    "\n",
    "    if len(df['Dataset'].str.startswith('task-1')) <= 0:\n",
    "        return \"\"\n",
    "    \n",
    "    rows_test = \"\"\n",
    "    for _, i in df[df['Dataset'] == 'task-1-type-classification-20221115-test'].iterrows():\n",
    "        rows_test += f'{i[\"Team\"]} & {i[\"Software\"].replace(\"TASK2: \", \"\").replace(\"TASK1: \", \"\").replace(\"Task1: \", \"\")} & {i[\"run_id\"]} '\n",
    "        for k in ['balanced-accuracy', 'precision-for-phrase-spoilers', 'recall-for-phrase-spoilers', 'f1-for-phrase-spoilers', 'precision-for-passage-spoilers', \n",
    "                  'recall-for-passage-spoilers', 'f1-for-passage-spoilers','precision-for-multi-spoilers', 'recall-for-multi-spoilers', 'f1-for-multi-spoilers']:\n",
    "            rows_test += f' & {fm(i, k)} '\n",
    "        rows_test += '\\\\\\\\\\n'\n",
    "\n",
    "    return \"\"\"\\\\begin{table*}[t]\n",
    "\\\\centering\n",
    "\\\\small\n",
    "\\\\renewcommand{\\\\tabcolsep}{3.5pt}\n",
    "\\\\caption{Overview of the effectiveness in spoiler type prediction (subtask~1 at SemEval~2023 Task~5) measured as balanced accuracy over all three spoiler types and precision (Pr.), recall (Rec.), and F1 score (F1) for phrase, passage, and multi spoilers on the test set. We report all runs by Team \"\"\" + team_name + \"\"\" and the baseline as well as one synthetic run that reports the best respectively the median observed scores for each measure.}\n",
    "\\\\label{table-effectiveness-task-1}\n",
    "\\\\begin{tabular}{@{}lllcccccccccc@{}}\n",
    "\\\\toprule\n",
    "  \\\\multicolumn{3}{c}{\\\\bfseries Submission} & \\\\bfseries Accuracy     &  \\\\multicolumn{3}{c}{\\\\bfseries Phrase} & \\\\multicolumn{3}{c}{\\\\bfseries Passage} & \\\\multicolumn{3}{c}{\\\\bfseries Multi}\\\\\\\\\n",
    "\\\\cmidrule(l@{\\\\tabcolsep}){1-3}\n",
    "\\\\cmidrule(l@{\\\\tabcolsep}){5-7}\n",
    "\\\\cmidrule(l@{\\\\tabcolsep}){8-10}\n",
    "\\\\cmidrule(l@{\\\\tabcolsep}){11-13}\n",
    "Team & Approach & Run & & Pr. & Rec. & F1 & Pr. & Rec. & F1 & Pr. & Rec. & F1 \\\\\\\\\n",
    "\n",
    "\\\\midrule\n",
    "\n",
    "\"\"\" + rows_test + \"\"\"\n",
    "\n",
    "\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{table*}\"\"\"\n",
    "\n",
    "def table_task_2(team_name):\n",
    "    df = f'/workspace/all-submissions/{team_name}/evaluation-task-2.jsonl'\n",
    "    if not os.path.isfile(df):\n",
    "        return \"\"\n",
    "    \n",
    "    df = pd.read_json(df, lines=True)\n",
    "\n",
    "    if len(df['Dataset'].str.startswith('task-2')) <= 0:\n",
    "        return \"\"\n",
    "    \n",
    "    rows = \"\"\n",
    "    measures = ['bleu-score-all-spoilers', 'bert-score-all-spoilers', 'meteor-score-all-spoilers',\n",
    "                'bleu-score-phrase-spoilers', 'bert-score-phrase-spoilers', 'meteor-score-phrase-spoilers',\n",
    "                'bleu-score-passage-spoilers', 'bert-score-passage-spoilers', 'meteor-score-passage-spoilers',\n",
    "                'bleu-score-multi-spoilers', 'bert-score-multi-spoilers', 'meteor-score-multi-spoilers']\n",
    "    \n",
    "    for _, i in df[df['Dataset'] == 'task-2-spoiler-generation-20221115-test'].iterrows():\n",
    "        rows += f'{i[\"Team\"]} & {i[\"Software\"].replace(\"TASK2: \", \"\").replace(\"TASK1: \", \"\")} & {i[\"run_id\"]} '\n",
    "        for k in measures:\n",
    "            rows += f' & {fm(i, k)} '\n",
    "        rows += '\\\\\\\\\\n'\n",
    "    \n",
    "    return \"\"\"\\\\begin{table*}[t]\n",
    "\\\\centering\n",
    "\\\\small\n",
    "\\\\renewcommand{\\\\tabcolsep}{3pt}\n",
    "\\\\caption{Overview of the effectiveness in spoiler generation (subtask~2 at SemEval~2023 Task~5) measured as BLEU-4 (BL4), BERTScore (BSc.) and METEOR (MET) over all clickbait posts respectively those requiring phrase, passage, or multi spoilers on the test set. We report all runs by Team \"\"\" + team_name + \"\"\" and the baseline as well as one synthetic run that reports the best respectively the median observed scores for each measure.}\n",
    "\\\\label{table-effectiveness-task-2}\n",
    "\\\\begin{tabular}{@{}lllcccccccccccccc@{}}\n",
    "\\\\toprule\n",
    "  \\\\multicolumn{3}{c}{\\\\bfseries Submission} & \\\\multicolumn{3}{c}{\\\\bfseries All}     &  \\\\multicolumn{3}{c}{\\\\bfseries Phrase} & \\\\multicolumn{3}{c}{\\\\bfseries Passage} & \\\\multicolumn{3}{c}{\\\\bfseries Multi}\\\\\\\\\n",
    "\\\\cmidrule(l@{\\\\tabcolsep}){1-3}\n",
    "\\\\cmidrule(l@{\\\\tabcolsep}){4-6}\n",
    "\\\\cmidrule(l@{\\\\tabcolsep}){7-9}\n",
    "\\\\cmidrule(l@{\\\\tabcolsep}){10-12}\n",
    "\\\\cmidrule(l@{\\\\tabcolsep}){13-15}\n",
    "Team & Approach & Run & BL4 & BSc. & MET & BL4 & BSc. & MET & BL4 & BSc. & MET & BL4 & BSc. & MET\\\\\\\\\n",
    "\n",
    "\\\\midrule\n",
    "\n",
    "\"\"\" + rows + \"\"\"\n",
    "\n",
    "\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{table*}\"\"\"\n",
    "\n",
    "def latex_code(team_name):\n",
    "    return \"\"\"\\\\pdfoutput=1\n",
    "\\\\documentclass[11pt]{article}\n",
    "\\\\usepackage{ACL2023}\n",
    "\\\\usepackage{times}\n",
    "\\\\usepackage{latexsym}\n",
    "\\\\usepackage{booktabs}\n",
    "\\\\usepackage[T1]{fontenc}\n",
    "\\\\usepackage[utf8]{inputenc}\n",
    "\\\\usepackage{microtype}\n",
    "\\\\usepackage{inconsolata}\n",
    "\n",
    "\\\\title{Results of \"\"\" + team_name + \"\"\" at SemEval-2023 Task 5}\n",
    "\\\\begin{document}\n",
    "\\\\maketitle\n",
    "\n",
    "The code that produces Table~\\\\ref{table-effectiveness-task-1} and Table~\\\\ref{table-effectiveness-task-2} is available at \\\\href{https://github.com/pan-webis-de/pan-code/tree/master/semeval23/generate-latex-tables.ipynb}.\n",
    "\"\"\" + table_task_1(team_name) + \"\"\"\n",
    " \n",
    "\n",
    "\n",
    "\"\"\" + table_task_2(team_name) + \"\"\"\n",
    "\n",
    "\\\\end{document}\n",
    "\"\"\"\n",
    "\n",
    "team_name = ''\n",
    "\n",
    "directory = f'/workspace/all-submissions/{team_name}/latex/'\n",
    "!mkdir -p {directory}\n",
    "!cp /workspace/all-submissions/acl2023.sty {directory}\n",
    "with open(f'{directory}/semeval23-clickbait-spoiling-effectiveness-tables.tex', 'w') as f:\n",
    "    f.write(latex_code(team_name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
